import imblearn
from imblearn.under_sampling import RandomUnderSampler
from imblearn.over_sampling import RandomOverSampler
from imblearn.under_sampling import TomekLinks
from imblearn.over_sampling import SMOTE
import pandas as pd
from lightgbm import LGBMClassifier  # Import LightGBM
from sklearn.metrics import accuracy_score, classification_report, precision_score, recall_score, f1_score
from sklearn.model_selection import train_test_split, GridSearchCV
import sys

remote_path = "results_lgbm.txt"

with open(remote_path, "w") as f:
    sys.stdout = f  # Redirect output to the file

    data = pd.read_csv('train_test_dataset.csv')
    df = pd.DataFrame(data)

    # Calculating the ratio of benign to malware APKs
    malware_column = df['Malware']
    ratio_ones_to_zeroes = malware_column.value_counts(normalize=True)
    print("Ratio of Benign APKs to Malware APKs:")
    print(ratio_ones_to_zeroes)

    # Set display options
    pd.set_option('display.max_rows', None)
    pd.set_option('display.max_columns', None)
    df = df.drop(['APK Name'], axis=1)

    # Print the names of all columns
    column_names = df.columns
    column_names_list = column_names.to_list()

    for col in column_names_list:
        df[col] = df[col].astype(int)

    # Split data into features and target
    X = df.drop(columns=['Malware'])  # Dropping the target
    y = df['Malware']

    # Split the data into training and testing sets
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, stratify=y, random_state=42)

    # Check the proportions in the training and testing sets
    print("Proportion of Malware in Training Set:")
    print(y_train.value_counts(normalize=True))
    print("Proportion of Malware in Testing Set:")
    print(y_test.value_counts(normalize=True))

    # Sampling procedures
    sampling_techniques = {
        "Random Under-sampling": RandomUnderSampler(random_state=42, replacement=True),
        # "Random Over-sampling": RandomOverSampler(random_state=42),
        "Tomek Links": TomekLinks(sampling_strategy='majority'),
        # "SMOTE": SMOTE()
    }

    lgb_model = LGBMClassifier(metric='binary_logloss', class_weight='balanced')  # Use 'binary_logloss' as the
    # evaluation metric

    # Define the parameter grid for LightGBM
    param_grid_lgb = {
        'n_estimators': [500],
        'max_depth': [6],
        'learning_rate': [0.001, 0.1],
        'subsample': [0.5, 0.8],
        'colsample_bytree': [1.0]
    }

    for technique_name, sampler in sampling_techniques.items():
        # Apply the sampling technique to the training set
        X_train_sampled, y_train_sampled = sampler.fit_resample(X_train, y_train)

        grid_search_lgb = GridSearchCV(lgb_model, param_grid_lgb, cv=10, verbose=2, n_jobs=-1)

        # Fit the model with the sampled training data
        grid_search_lgb.fit(X_train_sampled, y_train_sampled)

        # Predict and evaluate using the best model
        best_lgb = grid_search_lgb.best_estimator_
        y_pred_lgb = best_lgb.predict(X_test)

        print(f"================={technique_name} RESULTS===================")
        print("Best Parameters: ", grid_search_lgb.best_params_)
        print("Accuracy:", accuracy_score(y_test, y_pred_lgb))

        # Additional metrics
        print("Precision (Malware):", precision_score(y_test, y_pred_lgb, pos_label=1))
        print("Recall (Malware):", recall_score(y_test, y_pred_lgb, pos_label=1))
        print("F1 Score (Malware):", f1_score(y_test, y_pred_lgb, pos_label=1))

        # Classification report
        print(classification_report(y_test, y_pred_lgb, labels=[0, 1], target_names=["Benign", "Malware"]))

# Reset the output to the console
sys.stdout = sys.__stdout__

print("Results have been written to 'results.txt'")
